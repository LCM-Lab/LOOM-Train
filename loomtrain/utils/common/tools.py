from typing import Type, TypeVar, Callable, List, Tuple, Iterable
import os, random, itertools
from collections import defaultdict
from loomtrain.utils.common.iotools import save_pkl, read_pkl

CALLABLE = TypeVar("CALLABLE")

def Identity(x): return x

def chunks(lst: list, chunk_num: int):
    """Yield successive n-sized chunks from lst."""
    chunk_width = len(lst) // chunk_num
    ones = chunk_num - len(lst) % chunk_num 
    p = 0
    for i in range(chunk_num):
        if i == ones: chunk_width += 1
        yield lst[p: p + chunk_width]
        p += chunk_width



def counters(obj: Iterable, key: Callable, 
             mapping: Callable = Identity):
    dic = defaultdict(int)
    for k in obj:
        dic[mapping(key(k))] += 1
        
    return dict(dic)


def bucketize(obj: Iterable, bucket_size: int, 
              key: Callable = Identity,
              drop_exceed: bool = False, 
              shuffle: bool = False,
              seed: int = 42):
    '''
    Args:
        obj: An Iterable Object
        bucket_size: The max size one bucket can hold
        key: A function that return the `size` property of an element
        drop_exceed: If one element already exceed the bucket_size, drop it
        shuffle: whether using shuffled or not(sorted)
        seed: if shuffle, control the seed
    '''
    def I(x): return x[0]
    def S(x): return x[1]
    def SI(x): return x[1], x[0]

    indexed = [(i, key(d)) for i, d in enumerate(obj) \
               if (not drop_exceed) or key(d) <= bucket_size]

    if not shuffle: indexed.sort(key = SI)
    else:
        random.seed(seed)
        random.shuffle(indexed)
    
    buckets, indexs = [], []
    tmp_bucket, tmp_index, tmp_size = [], [], 0 
    for x in indexed:
        if tmp_size + S(x) > bucket_size:
            buckets += [tmp_bucket]
            indexs += [tmp_index]

            tmp_bucket, tmp_index, tmp_size = [S(x)], [I(x)], S(x)
        else: 
            tmp_bucket += [S(x)]
            tmp_index += [I(x)]
            tmp_size += S(x)
    
    return buckets, indexs









def randint(a: int, b: int) -> int:
    return random.randint(a, b)

def random_sample(population, k: int) -> List[int]:
    return random.sample(population, k)

def shuffle(lst: list) -> list:
    random.shuffle(lst)
    return lst


def C(m: int, n: int) -> list:
    return list(itertools.combinations(range(m), n))



def cached(target: Type[CALLABLE],
           kwargs: dict = dict(),
           cache_dir: str = None,
           cache_name: str = None) -> CALLABLE:
    if not cache_dir: cache_dir = '.'
    if not cache_name:
        cache_name = "-".join([
            f"{k}={v}" for k, v in kwargs.items()
        ])

    cache_path = f"{cache_dir}/{cache_name}.pkl"
    if os.path.exists(cache_path):
        return read_pkl(cache_path)
    obj = target(**kwargs)
    save_pkl(obj, cache_path)
    return obj

def locatedlog(*args, **kwargs):
    import inspect
    frame = inspect.stack()[1]
    filename = frame.filename
    lineno = frame.lineno
    print(f"[{filename}:{lineno}]\n", 
          *args, **kwargs)

def mapping(objs: Iterable, 
            worker: Callable, 
            tool: Callable = lambda :tuple(),
            num_processes: int = 8):
    '''tools : a function building global variables costing time, like tokenizer'''
    from loomtrain import Marks
    from tqdm import tqdm
    import multiprocessing as mp
    def mapping_worker(wk, tl, get_queue: mp.Queue, to_queue: mp.Queue): 
        t = tl() 
        while True:
            obj = get_queue.get()
            if obj == Marks.stop: break
            to_queue.put(wk(obj, *t))

    to_queue = mp.Queue()
    get_queue = mp.Queue()

    processes = []
    for i in range(num_processes):
        p = mp.Process(target=mapping_worker,
                       args=(worker, tool, to_queue, get_queue))
        p.start()
        processes += [p]

    total, ret = 0, []
    for obj in tqdm(objs, desc = "Preparing"): 
        total += 1
        to_queue.put(obj)
    for _ in range(num_processes): to_queue.put(Marks.stop)
    for _ in tqdm(range(total), desc = "Mapping"): ret += [get_queue.get()]
    for p in processes: p.join()

    return ret
    



def filtering(objs: Iterable, 
              filter: Callable, 
              tool: Callable = lambda :tuple(), 
              num_processes: int = 8):
    '''tools : a function building global variables costing time, like tokenizer'''
    from loomtrain import Marks
    from tqdm import tqdm
    import multiprocessing as mp
    def filtering_worker(ft, tl, get_queue: mp.Queue, to_queue: mp.Queue): 
        t = tl() 
        while True:
            obj = get_queue.get()
            if obj == Marks.stop: break
            if ft(obj, *t): to_queue.put(obj)
            else: to_queue.put(Marks.stop)

    to_queue = mp.Queue()
    get_queue = mp.Queue()

    processes = []
    for i in range(num_processes):
        p = mp.Process(target=filtering_worker,
                       args=(filter, tool, to_queue, get_queue))
        p.start()
        processes += [p]

    total, ret = 0, []
    for obj in tqdm(objs, desc = "Preparing"): 
        total += 1
        to_queue.put(obj)
    for _ in range(num_processes): to_queue.put(Marks.stop)
    for _ in tqdm(range(total), desc="Filtering"): 
        obj = get_queue.get()
        if obj != Marks.stop: ret += [obj]
    for p in processes: p.join()

    return ret
    
